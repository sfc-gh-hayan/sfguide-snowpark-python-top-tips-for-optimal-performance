{
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9434343805f944cca68c9baa90b0479ced9875b0751064f0bea749b2255bd0b5"
   }
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Lab 3 - Examining the performance benefits of the Cachetools library\n\n### Setup\n\nBelow are the imports needed for Lab 3 - Cachetools and have been included in the Anaconda packages provided by Snowflake",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell2",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf, avg, col,lit,call_udf,min,call_builtin,call_function,call_udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType, BooleanType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": [
    "Next, import snowpark and verify by printing the version."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell4",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import version\n",
    "print(version.VERSION)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Connect to active Snowflake session.",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell6",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Testing the session\nsession.sql('use database snowpark_best_practices_labs')\nprint(session.sql('select current_warehouse(), current_database(), current_schema()').collect())\n",
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Confirm that the the output of the print statement is the following: `[Row(CURRENT_WAREHOUSE()='COMPUTE_WH', CURRENT_DATABASE()='SNOWPARK_BEST_PRACTICES_LAB', CURRENT_SCHEMA()='PUBLIC')]`\n\n",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "### Create the Pickle File\n",
    "Run the following code to build the sample pickle file."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell9",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "day_dict = {1: 'monday', 2: 'tuesday', 3: 'wednesday', 4: 'thursday', 5: 'friday', 6: 'saturday', 7: 'sunday'}\n",
    "print(day_dict)\n",
    "\n",
    "with open('alldays.pkl', 'wb') as file:\n",
    "    pickle.dump(day_dict, file) "
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "Next, let's test the pickle file output."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell11",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample function to test loading pickle file\n",
    "def getname():\n",
    "    with open('alldays.pkl','rb') as fileopen:\n",
    "        f1=pickle.load(fileopen)               \n",
    "    return f1\n",
    "\n",
    "def getday(key):\n",
    "    dict1=getname()\n",
    "    return dict1[key]\n",
    "\n",
    "r=getday(3)\n",
    "print(r)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "### Creating named Stage and Uploading the Pickle File\n",
    "\n",
    "Next, we will create a stage in Snowflake to upload the pickle file to."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell13",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.sql(\"create or replace stage pythonstage\").collect()\n",
    "session.file.put(\"alldays.pkl\", \"pythonstage\", auto_compress=False,overwrite=True)\n",
    "session.sql(\"ls @pythonstage\").collect()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "With the pickle file successfully loaded, we are now ready to A/B test our UDF with and without Cachetools in the next steps."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "### Creating Python UDF Without Cachetools\n",
    "\n",
    "The block of code below creates a Python UDF without any use of the Cachetools library."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell16",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import StringType,IntegerType\n",
    "\n",
    "'''\n",
    "We are importing the Pickle file in the session.\n",
    "\n",
    "extract_name() -> deserializes the dictionary from the pickle and returns a dictionary object to the caller\n",
    "\n",
    "getvalue() -> Takes a day number and return the name.\n",
    "\n",
    "'''\n",
    "session.add_import(\"@pythonstage/alldays.pkl\")\n",
    "def extract_name()->dict:\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    file_path = import_dir + \"alldays.pkl\"\n",
    "    \n",
    "    with open(file_path,'rb') as file:\n",
    "        dict1=pickle.load(file)\n",
    "\n",
    "    return dict1\n",
    "\n",
    "def getvalue(key:int)->str:   \n",
    "    filedict= extract_name()\n",
    "    return filedict[key]\n",
    "    \n",
    "# Creating a Python UDF\n",
    "udf_nocache = session.udf.register(\n",
    "    func=getvalue,\n",
    "    name=\"udf_nocachetools\",\n",
    "    stage_location='pythonstage',\n",
    "    is_permanent=True,\n",
    "    replace=True, \n",
    "    input_types=[IntegerType()],\n",
    "    return_type=StringType()\n",
    ")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "### Creating Sample data\n",
    "Next, we will create 2 million rows of sample data for the UDF to run against."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell18",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_random = np.random.randint(low=1, high=7, size=(2000000,3))\n",
    "df = pd.DataFrame(arr_random, columns=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions=session.createDataFrame(df,schema=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions.count() # 2 Million records"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "### Call the UDF\n",
    "Let's call the UDF and create a new table from the resultset with the below code."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st=datetime.now()\n",
    "df_transactions.withColumn('weekdayname',call_udf('udf_nocachetools',df_transactions['\"weekday\"'].astype('int')))\\\n",
    ".write.mode('overwrite').save_as_table(\"NoCacheTransactionTable\")\n",
    "et=datetime.now()\n",
    "print(f\"Total duration without using Cachetools library ={(et-st).total_seconds()}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "For the first run you will likely see better performance, but when you execute it multiple times, you will start seeing query taking more time to complete. Note your runtime after running 4-5 times. It will likely be between 20-25 seconds.",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "id": "da66b345-c2f9-459f-b02f-e0753eefca05",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "### Analyze performance with the Query Profiler\nLet's take a look at the query profile in Snowflake to understand where the time is spent.  Login to Snowsight and navigate to [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity#label-snowsight-activity-query-history) and click on the query profile for the last query ran.  Observate total Python UDF handler execution time. During query execution, for every record the UDF invocation is reading the file from disk."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "Lastly, confirm that the UDF ran and successfully created a new table as expected by running the following:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell24",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.sql(\"select * from NoCacheTransactionTable limit 10\").show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "### Creating Python User Defined Function With Cachetools\n",
    "\n",
    "In the below cell, we will leverage the [Cachetools](https://pypi.org/project/cachetools/) library which will read the pickle file once and cache it."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell26",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import StringType\n",
    "import zipfile\n",
    "\n",
    "'''\n",
    "Using cachetools decorator while creating the function extract_name. Using this decorator the file will be read once and then cached. \n",
    "Other UDF execution will use the cached file and avoids reading the file from the storage.\n",
    "'''\n",
    "\n",
    "session.add_import(\"@pythonstage/alldays.pkl\")\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def extract_name()->dict:\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    file_path = import_dir + \"alldays.pkl\"\n",
    "    \n",
    "    with open(file_path,'rb') as file:\n",
    "        dict1=pickle.load(file)\n",
    "\n",
    "    return dict1\n",
    "\n",
    "\n",
    "def getvalue(key:int)->str:   \n",
    "    filedict= extract_name()\n",
    "    return filedict[key]\n",
    "    \n",
    "\n",
    "session.add_packages(\"cachetools\")\n",
    "udf_cache = session.udf.register(\n",
    "    func=getvalue,\n",
    "    name=\"udf_withcachetools\",\n",
    "    stage_location='pythonstage',\n",
    "    is_permanent=True, \n",
    "    replace=True, \n",
    "    input_types=[IntegerType()],\n",
    "    return_type=StringType()\n",
    ")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "### Creating Sample data\n",
    "\n",
    "Just like in the previous test, we will create 2 million rows of sample data for the UDF to run against."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell28",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_random = np.random.randint(low=1, high=7, size=(2000000,3))\n",
    "df = pd.DataFrame(arr_random, columns=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions=session.createDataFrame(df,schema=['invoice_num','trx_num','weekday'])\n",
    "\n",
    "df_transactions.count() # 2 Million records"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell29"
   },
   "source": [
    "### Call the UDF\n",
    "\n",
    "Let's call the UDF and create a new table from the resultset with the below code."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell30",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st=datetime.now()\n",
    "df_transactions.withColumn('\"weekdayname\"',call_udf('udf_withcachetools',df_transactions['\"weekday\"'].astype('int')))\\\n",
    ".write.mode('overwrite').save_as_table(\"CacheToolsTransactionTable\")\n",
    "et=datetime.now()\n",
    "print(f\"Total duration ={(et-st).total_seconds()}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "markdown",
   "id": "9021905e-51fb-4e3b-a874-56243ffcce8d",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "Note the runtime - it should be around 4-6 seconds, even if you rerun it multiple times.\n\n### Analyze performance with the Query Profiler\n\nLets look at the query profile to understand where the time is spent. We should be able to clearly see that the Total Python UDF handler execution time is around ~50% as the UDF invocation is reading the the pickle content from the dictionary."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "### Cleanup\n\nDelete the dataframe and drop the tables created by the UDF executions in the A/B testing steps.",
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell33",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Deleting the dataframe\ndel df_transactions\n\n# Dropping the table\nsession.sql(\"drop table CacheToolsTransactionTable\").collect()\nsession.sql(\"drop table NoCacheTransactionTable\").collect()\n",
   "id": "ce110000-1111-2222-3333-ffffff000032"
  }
 ]
}
