{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e026f6e58e7e7fcac433b20e0e98bc28079064f8b39889c46bb1bee90d36b9d"
   }
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Lab 2 - Vectorized UDFs for Batching\n\n### Setup\n\nBelow are the imports needed for this lab.  They have been included in the Anaconda packages provided by Snowflake",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell2",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing snowpark libraries\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import avg, stddev, udf\n",
    "from snowflake.snowpark.types import FloatType, StringType, PandasSeries\n",
    "\n",
    "# Others\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "Next, import snowpark and verify by printing the version.",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "id": "c580a02c-116a-4fa6-a952-bfce5839bd08",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import version\nprint(version.VERSION)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a82ec4d-aae0-4020-800f-274711c20c93",
   "metadata": {
    "name": "cell51",
    "collapsed": false
   },
   "source": "Connect to active Snowflake session."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell4",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "#This is the same as in previous sections of this lab\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n\n# Testing the session\nsession.sql(\"CREATE DATABASE IF NOT EXISTS SNOWPARK_BEST_PRACTICES_LABS\")\nsession.sql(\"SELECT current_warehouse(), current_database(), current_schema()\").show()",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "Before continuing, we will disable caching to ensure proper performance comparisons.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell6",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "### Create Dataframes\n",
    "\n",
    "As mentioned earlier, our data (TPCH dataset) is available from Snowflake via an inbound data share. We are taking 4 datasets into consideration, namely, Customers & Orders with 2 different sizes.\n",
    "\n",
    "Let's create all of these datasets as dataframes."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell8",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_customer_100 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer\")\n",
    "df_customer_1000 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer\")\n",
    "df_order_100 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.orders\")\n",
    "df_order_1000 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.orders\")\n",
    "df_customer_100.limit(2).show()\n",
    "df_order_100.limit(2).show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "Next, let's check on the data volumes we will be working with.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell10",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"df_customer_100 = {str(df_customer_100.count()/1000000)} M, df_customer_1000 = {str(df_customer_1000.count()/1000000)} M,\\ndf_order_100 = {str(df_order_100.count()/1000000)} M, df_order_1000 = {str(df_order_1000.count()/1000000)} M\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "### Setup Warehouses\n",
    "As mentioned earlier, we will be using 2 different Warehouse Sizes:\n",
    "\n",
    "For *_100 datasets, we will use Small and Medium Warehouse sizes\n",
    "For *_1000 datasets, we will use Medium and Large Warehouse sizes"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell12",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's change the warehouse size as required\n",
    "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\n",
    "\n",
    "# Let's check the current size of the warehouse\n",
    "session.sql(\"SHOW WAREHOUSES LIKE '%COMPUTE_WH%'\").collect()\n",
    "session.sql('SELECT \"name\", \"size\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))').show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "### Use Case 1: Numeric Computation\nFor this use case, we will make use of the Customer datasets. Let's first compute the mean and standard deviation for Account Balance in Customer datasets.\n\nYou may notice depreciation warning messages , which are safe to ignore at this time.",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell14",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## df_customer_100\n",
    "df_customer_100_mean = float(df_customer_100.agg(avg(\"C_ACCTBAL\")).to_pandas().values[0])\n",
    "df_customer_100_stddev = float(df_customer_100.agg(stddev(\"C_ACCTBAL\")).to_pandas().values[0])\n",
    "\n",
    "## df_customer_1000\n",
    "df_customer_1000_mean = float(df_customer_1000.agg(avg(\"C_ACCTBAL\")).to_pandas().values[0])\n",
    "df_customer_1000_stddev = float(df_customer_1000.agg(stddev(\"C_ACCTBAL\")).to_pandas().values[0])"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "Let's create 2 sets of UDFs, one for each dataset as the means & stdevs will be different\n",
    "\n",
    "- As mentioned earlier, these are hypothetical usecases\n",
    "- In this UDF, we take the Customer Account Balance as Input, subtract with the Mean, add the Standard Deviation, and finally multiple with 10,000 Notice how we are creating a Normal and Vectorised UDF both"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell16",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## df_customer_100\n",
    "def basic_compute_100(inp):\n",
    "    return (inp - df_customer_100_mean + df_customer_100_stddev) * 10000.0\n",
    "\n",
    "# Let's create a UDF on top of that\n",
    "udf_bc_100 = udf(basic_compute_100, return_type=FloatType(), input_types=[FloatType()])\n",
    "\n",
    "# Let's vectorise the UDF\n",
    "@udf()\n",
    "def vect_udf_bc_100(inp: PandasSeries[float]) -> PandasSeries[float]:\n",
    "    return (inp - df_customer_100_mean + df_customer_100_stddev) * 10000.0"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "The above is an example of basic_compute_100 function created as a Temporary UDF within Snowflake. It takes a few seconds to create the UDF, the first time. Subsequent runs will take less time\n",
    "\n",
    "- You'll notice the run time parameters such as `Python version=3.8, Cloudpickle==2.0.0` package, and so on\n",
    "- More importantly towards the end, you can see the actual function definition\n",
    "\n",
    "Using @udf with a Pandas Series or Dataframe as input will automatically make use of the Python UDF Batch API or Vectorised UDF. \n",
    "The UDF definition for a Vectorised UDF looks pretty much the same as for a normal non-vectorised one However, the key differences apart from our definition for `vect_udf_bc100` is:\n",
    "\n",
    "- Importing the pandas library: this can be seen towards the end of the Vectorised UDF definition\n",
    "- The Vectorised input argument: `compute._sf_vectorized_input = pandas.DataFrame`"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell18",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## df_customer_1000\n",
    "def basic_compute_1000(inp):\n",
    "    return (inp - df_customer_1000_mean + df_customer_1000_stddev) * 10000.0\n",
    "\n",
    "# Let's create a UDF on top of that\n",
    "udf_bc_1000 = udf(basic_compute_1000, return_type=FloatType(), input_types=[FloatType()])\n",
    "\n",
    "# Let's vectorise the UDF\n",
    "@udf()\n",
    "def vect_udf_bc_1000(inp: PandasSeries[float]) -> PandasSeries[float]:\n",
    "    return (inp - df_customer_1000_mean + df_customer_1000_stddev) * 10000.0"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "Let's quickly execute these UDFs against our account balance datasets 100 & 1000. In our methodology we had discussed using different Virtual Warehouse sizes:\n\n- Small and Medium for 100 dataset\n- Medium and Large for 1000 dataset \n\nWe will be re-running this accordingly",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nprint(\"Use case 1.1: Numeric Computation with Small WH and 100x dataset\")\n\nst=datetime.now()\ndf_customer_100.select(udf_bc_100(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_customer_100.select(vect_udf_bc_100(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 1.2: Numeric Computation with Medium WH and 100x dataset\")\n\nst=datetime.now()\ndf_customer_100.select(udf_bc_100(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_100.select(vect_udf_bc_100(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 1.3: Numeric Computation with Medium WH and 1000x dataset\")\n\nst=datetime.now()\ndf_customer_1000.select(udf_bc_1000(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_1000.select(vect_udf_bc_1000(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nprint(\"Use case 1.4: Numeric Computation with Large WH and 1000x dataset\")\nst=datetime.now() \ndf_customer_1000.select(udf_bc_1000(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_1000.select(vect_udf_bc_1000(\"C_ACCTBAL\").alias(\"bal_from_mean\")).agg(avg(\"bal_from_mean\")).show()\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell24"
   },
   "source": [
    "### Use Case 2: String Manipulation\n",
    "To test a workload based on string manipulation, let's create a single set of UDFs for Customer100 & Customer1000 datasets. Here, we take the Customer Name as input and split based on the â€˜#' character. As before, we will create a Normal and Vectorised UDF"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell25",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str_manipulate(inp):\n",
    "    return inp.split('#')[1]\n",
    "\n",
    "# Let's create a UDF on top of that\n",
    "udf_sm = udf(str_manipulate, return_type=StringType(), input_types=[StringType()])\n",
    "\n",
    "# Let's vectorise the same UDF\n",
    "@udf()\n",
    "def vect_udf_sm(inp: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    return inp.str.split('#', expand=True)[1]"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "As before, let's quickly execute these UDFs against our datasets 100 & 1000. We will re-run these UDFs based on our Dataset and VW sizing"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell27",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nprint(\"Use case 2.1: String Manipulation with Small WH and 100x dataset\")\nst=datetime.now() \ndf_customer_100.select(udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_100.select(vect_udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell28",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 2.2: String Manipulation with Medium WH and 100x dataset\")\nst=datetime.now() \ndf_customer_100.select(udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_100.select(vect_udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell29",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 2.3: String Manipulation with Medium WH and 1000x dataset\")\nst=datetime.now() \ndf_customer_1000.select(udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_customer_1000.select(vect_udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell30",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nprint(\"Use case 2.4: String Manipulation with Large WH and 1000x dataset\")\nst=datetime.now()\ndf_customer_1000.select(udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_customer_1000.select(vect_udf_sm(\"C_NAME\").alias(\"CustIDs\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.CustIDs\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell31"
   },
   "source": [
    "### Use Case 3: Regex Masking\n",
    "Another common use case is Regex Masking. To set this up, let's create a single set of UDFs for Customer100 & Customer1000 datasets. Here, we take the Customer Phone Number as input and masks the last 4 digits. As before, we will create a Normal and Vectorised UDF."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell32",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a slightly complex function using native python regex for string replacement/masking\n",
    "def mask_data(inp):\n",
    "    return re.sub('\\d{4}$', '****', inp)\n",
    "\n",
    "# Let's create a UDF on top of that\n",
    "udf_md = udf(mask_data, return_type=StringType(), input_types=[StringType()])\n",
    "\n",
    "# Let's vectorise the same UDF\n",
    "@udf()\n",
    "def vect_udf_md(inp: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    return inp.apply(lambda x: mask_data(x))"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell33"
   },
   "source": [
    "As before, let's quickly execute these UDFs against our datasets 100 & 1000. We will re-run these UDFs based on our Dataset and VW sizing"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell34",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nprint(\"Use case 3.1: Regex Masking with Small WH and 100x dataset\")\nst=datetime.now()\ndf_customer_100.select(udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_customer_100.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell35",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 3.2: Regex Masking with Medium WH and 100x dataset\")\nst=datetime.now()\ndf_customer_100.select(udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_customer_100.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell36",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 3.3: Regex Masking with Medium WH and 1000x dataset\")\nst=datetime.now()\ndf_customer_1000.select(udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_1000.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell37",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nprint(\"Use case 3.4: Regex Masking with Large WH and 1000x dataset\")\nst=datetime.now() \ndf_customer_1000.select(udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_1000.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell38"
   },
   "source": [
    "To try another approach, let's vectorise the original UDF again, this time using the Pandas replace code.  After creation, check the performance against the two datasets.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell39",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def vect_udf_md(inp: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    return inp.replace(to_replace ='\\d{4}$', value = '****', regex = True)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell40",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 3.5: Regex Masking with Pandas replace, using Medium WH and 100x then 1000x dataset\")\nst=datetime.now() \ndf_customer_100.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF on 100x:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_customer_1000.select(vect_udf_md(\"C_PHONE\").alias(\"masked_phone_nums\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.masked_phone_data\")\net=datetime.now()\nprint(f\"Vectorized UDF on 1000x:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell41"
   },
   "source": [
    "### Timestamp Manipulation\n",
    "The final example is changing a timestamp field. Let's create a single set of UDFs for Order100 & Order1000 datasets. Here, we take the Order Data as input and apply some basic Timestamp operations. As before, we will create a Normal and Vectorised UDF."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell42",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Let's try some timestamp manipulation\n",
    "def change_format(inp):\n",
    "    instantiate = datetime.strptime(inp, '%Y-%m-%d')\n",
    "    change_format = datetime.strptime(instantiate.strftime('%m/%d/%Y'), '%m/%d/%Y')\n",
    "    dt_add3_days = change_format + timedelta(days=10)\n",
    "    return dt_add3_days\n",
    "\n",
    "# Let's create a UDF on top of that\n",
    "udf_cf = udf(change_format, return_type=StringType(), input_types=[StringType()])\n",
    "\n",
    "# Let's vectorise the UDF - still using native python\n",
    "@udf()\n",
    "def vect_udf_cf(inp: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    return inp.apply(lambda x: change_format(x))"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000041"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell43"
   },
   "source": [
    "As before, let's execute these UDFs against our datasets 100 & 1000. We will re-run these UDFs based on our Dataset and VW sizing.  \n",
    "\n",
    "**NOTE:**  **This use case is the most computationally intensive and will take the longest compared to the previous use cases.**\n",
    "The 1000 datasets take ~30 minutes to run each, so please be cognizant of credit usage and feel free to skip ahead to the analysis if desired. "
   ],
   "id": "ce110000-1111-2222-3333-ffffff000042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell44",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nprint(\"Use case 4.1: Timestamp Manipulation with Small WH and 100x dataset\")\nst=datetime.now() \ndf_order_100.select(udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()  \ndf_order_100.select(vect_udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000043"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell45",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 4.2: Timestamp Manipulation with Medium WH and 100x dataset\")\nst=datetime.now() \ndf_order_100.select(udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_order_100.select(vect_udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000044"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell46",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nprint(\"Use case 4.3: Timestamp Manipulation with Medium WH and 1000x dataset\")\nst=datetime.now() \ndf_order_1000.select(udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now()\ndf_order_1000.select(vect_udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell47",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nprint(\"Use case 4.4: Timestamp Manipulation with Large WH and 1000x dataset\")\nst=datetime.now()\ndf_order_1000.select(udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Scalar UDF:={(et-st).total_seconds()}\")\n\nst=datetime.now() \ndf_order_1000.select(vect_udf_cf(\"O_ORDERDATE\").alias(\"NY_OrderDate\")).write.mode(\"overwrite\").save_as_table(\"SNOWPARK_BEST_PRACTICES_LABS.PUBLIC.new_order_dates\")\net=datetime.now()\nprint(f\"Vectorized UDF:={(et-st).total_seconds()}\")",
   "id": "ce110000-1111-2222-3333-ffffff000046"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "name": "cell48",
    "collapsed": false
   },
   "source": "### Cleanup\n\nThe following code block cleans up what was executed in this lab, by accomplishing the following:\n - Resetting the warehouse to SMALL\n - Drop and recreate the database to quickly remove all tables created from this lab.  ",
   "id": "ce110000-1111-2222-3333-ffffff000047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell49",
    "language": "python"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nsession.sql(\"DROP DATABASE SNOWPARK_BEST_PRACTICES_LABS\")\nsession.sql(\"CREATE DATABASE SNOWPARK_BEST_PRACTICES_LABS\")",
   "id": "ce110000-1111-2222-3333-ffffff000048"
  }
 ]
}