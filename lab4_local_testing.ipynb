{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source data setup\n",
    "\n",
    "Before you begin this lab, please make sure you've gone through this [additional README](test_integration/README.md).\n",
    "\n",
    "This demo uses a free dataset available on Snowflake Marketplace called ['United States Retail Foot Traffic Data'](https://app.snowflake.com/marketplace/listing/GZT1ZVTYF7/constellation-network-united-states-retail-foot-traffic-data?search=us%20foot%20traffic).\n",
    "Please ensure that you have installed that dataset within your account before proceeding.\n",
    "\n",
    "Also ensure that you have appropriate grants on the Database and Schema to be able to create permanent and/or temporary tables in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scenario\n",
    "\n",
    "In this demo we will try to build an anomaly dectection model using a sample timeseries from a Marketplace provider called ['United States Retail Foot Traffic Data'](https://app.snowflake.com/marketplace/listing/GZT1ZVTYF7/constellation-network-united-states-retail-foot-traffic-data?search=us%20foot%20traffic). This timeseries represents average footfall in different states across US with regions of interest to retail stores. \n",
    "\n",
    "Our aim would be to understand if there's any abnormal footfall behaviour across the state of CA (focussing on single series for this demo) which could need more careful analysis/epxloration.\n",
    "\n",
    "The data we receive is actually in JSON and therefore will need some pre-processing before it can be supplied to SNOWFLAKE.ML.ANOMALY_DETECTION procedure to train a model on. The model that gets trained on this pre-processing data, can then be used to identify anomalies over a test dataset.\n",
    "\n",
    "## Application structure\n",
    "This entire application is written using Snowpark API and can be run from the client side orchestration tooling. All the data prep transformations, model training & inferencing are written into Python modules respectively.\n",
    "\n",
    "```\n",
    " test_integration/\n",
    " |--foot_traffic_data_prep.py\n",
    " |--foot_traffic_anomaly_detection.py\n",
    "```\n",
    "\n",
    "To this set we'll add some basic tests using the Pytest framework to test data prep transformers locally, and then the model training & inferencing on Snowflake (Anomaly Detection function can be run on Snowflake). The complete code source base looks like:\n",
    "\n",
    "```\n",
    " test_integration/\n",
    " |--foot_traffic_data_prep.py\n",
    " |--foot_traffic_anomaly_detection.py\n",
    " |--conftest.py\n",
    " |--test_foot_traffic_data_prep.py\n",
    " |--test_foot_traffic_anomaly_detection.py\n",
    " ```\n",
    "\n",
    " ## Local Testing Framework\n",
    "\n",
    "We can use the Local testing framework to quickly validate these data prep transformers without necessarily spending compute to test againt Snowflake. We'll use a representative set of JSON records based on the actual data sample from the chosen provider.\n",
    "\n",
    "Local testing framework provides a new config parameter called 'local_testing' while creating a new Session. All the supported APIs can be tested locally as they're used within transforming functions. For those that aren't supported you could write mock patch to simulate the behaviour of that API. \n",
    "\n",
    "This test suite is programmed for the following:\n",
    "- [x] Local test configuration\n",
    "- [x] Fixture for Snowpark Session\n",
    "- [x] Fixture for sample data creation\n",
    "- [x] Skip tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def pytest_addoption(parser):\n",
      "    parser.addoption(\"--snowflake-session\", action=\"store\", default=\"live\", help=\"--snowflake-session [local|live]\") ##live represents Snowflake connection\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat test_integration/conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.9, pytest-7.4.4, pluggy-1.0.0\n",
      "rootdir: /Users/hayan/Documents/GitHub/snowpark-best-practices-v2-main\n",
      "plugins: anyio-4.2.0\n",
      "collected 3 items\n",
      "\n",
      "test_integration/test_foot_traffic_anomaly_detection.py \u001b[33ms\u001b[0m\u001b[33m                [ 33%]\u001b[0m\n",
      "test_integration/test_foot_traffic_data_prep.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1 skipped\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 1.21s\u001b[0m\u001b[33m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "pytest test_integration/ --disable-warnings --snowflake-session local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.9, pytest-7.4.4, pluggy-1.0.0\n",
      "rootdir: /Users/hayan/Documents/GitHub/snowpark-best-practices-v2-main\n",
      "plugins: anyio-4.2.0\n",
      "collected 3 items\n",
      "\n",
      "test_integration/test_foot_traffic_anomaly_detection.py \u001b[32m.\u001b[0m\u001b[33m                [ 33%]\u001b[0m\n",
      "test_integration/test_foot_traffic_data_prep.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m======================== \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m4 warnings\u001b[0m\u001b[33m in 26.01s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## Running against live session \n",
    "## Live testing helps identify potential issues by running on a large sample of data on Snowflake but also make sure any objects that need to be created, like through a Stored Procedure, are created as expected.\n",
    "pytest test_integration/ --disable-warnings --snowflake-session live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
