{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Lab 3 - Vectorized UDTFs for Batching\n\n### Setup\n\nBelow are the imports needed for this lab.  They have been included in the Anaconda packages provided by Snowflake\n"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "source": "# Importing snowpark libraries\nimport snowflake.snowpark\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark.functions import avg, stddev, udf\nfrom snowflake.snowpark.types import FloatType, StringType, PandasSeries\n\n# Others\nimport json\nimport re\nimport pandas as pd\nfrom datetime import datetime, timedelta",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "cell3"
   },
   "source": "Next, import snowpark and verify by printing the version."
  },
  {
   "cell_type": "code",
   "id": "1c0a9cd1-3003-4616-ad18-d7e30e4d364b",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import version\nprint(version.VERSION)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5912710-5c9c-43f3-96c1-52db963b93a0",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Connect to active Snowflake session."
  },
  {
   "cell_type": "code",
   "id": "207f85d1-6a78-4b48-b1e3-477603333f81",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "#This is the same as in previous sections of this lab\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n\n# Testing the session\nsession.sql(\"CREATE DATABASE IF NOT EXISTS SNOWPARK_BEST_PRACTICES_LABS\")\nsession.sql(\"SELECT current_warehouse(), current_database(), current_schema()\").show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a308cf07-0bb8-48c7-a2fe-1defe9bc2cab",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Before continuing, we will disable caching to ensure proper performance comparisons."
  },
  {
   "cell_type": "code",
   "id": "6e0d06ab-fd8e-408b-983a-3ffea236ce4a",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63ec79ed-f173-468c-bd23-b88fea32a7ae",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Create Dataframes\n\nAs mentioned earlier, our data (TPCH dataset) is available from Snowflake via an inbound data share. We are taking Customers datasets into consideration with 2 different sizes.\n\nLet's create all of these datasets as dataframes."
  },
  {
   "cell_type": "code",
   "id": "ec4e9fc1-e088-401a-b7bd-e17a5cca9095",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_customer_100 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer\")\ndf_customer_1000 = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer\")\ndf_customer_100.limit(2).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6347812e-2195-4aa0-bf17-544eed8b04d7",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "Next, let's check on the data volumes we will be working with."
  },
  {
   "cell_type": "code",
   "id": "edb012d5-70ab-4e42-8c5d-9aa32bbed59a",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's quickly check how many records we are going to play with\nprint(f\"df_customer_100 = {str(df_customer_100.count()/1000000)} M, df_customer_1000 = {str(df_customer_1000.count()/1000000)} M\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2046a84-47ac-413b-9693-c300792240d3",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "### Setup warehouse\n- As mentioned earlier, we will be using 2 different Warehouse Sizes:\n    - For *_100 datasets, we will use Small and Medium Warehouse sizes\n    - For *_1000 datasets, we will use Medium and Large Warehouse sizes"
  },
  {
   "cell_type": "code",
   "id": "30550017-861d-4de9-a87d-4c1c5e13452a",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's change the warehouse size as required\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\n\n# Let's check the current size of the warehouse\nsession.sql(\"SHOW WAREHOUSES LIKE 'COMPUTE_WH'\").collect()\nsession.sql('SELECT \"name\", \"size\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))').show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "462b3250-95a1-42e5-addb-8cbd14a54fc2",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "### Usecase 1: Numerical Computation using Regular UDTF\n\n- We have a simple numerical computation use case wherein, we compute the mean and standard deviation using a regular UDTF.\n- In addition, we are also checking for NULL values and ensuring they are removed in order to get the right results."
  },
  {
   "cell_type": "code",
   "id": "260a9b8c-9525-4308-ae0a-35b4f9b69411",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_udtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (mean_acctbal NUMBER(12, 2), stddev_acctbal NUMBER(12, 2))\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy')\nHANDLER='ComputeMeanStddevUDTF'\nAS $$\nimport numpy as np\n\nclass ComputeMeanStddevUDTF:\n    def __init__(self):\n        self.acctbal_values = []\n\n    def process(self, c_acctbal):\n        # Collect each row's c_acctbal value\n        self.acctbal_values.append(float(c_acctbal) if c_acctbal is not None else np.nan)\n\n    def end_partition(self):\n        # Convert to numpy array and handle NaN values\n        c_acctbal = np.array(self.acctbal_values)\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            yield (np.nan, np.nan)\n        else:\n            # Compute mean\n            mean_acctbal = np.mean(c_acctbal)\n            # Compute standard deviation\n            stddev_acctbal = np.std(c_acctbal)\n            yield (round(mean_acctbal, 2), round(stddev_acctbal, 2))\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0b46269-3d86-4901-88d0-f32dd304ee8c",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "collapsed": false
   },
   "outputs": [],
   "source": "# First, let's get the results of the UDTF\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_udtf.mean_acctbal, compute_mean_stddev_udtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_udtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c830177-5ecd-49ec-b5ba-c7dee5d691dd",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "collapsed": false
   },
   "outputs": [],
   "source": "\n# Now let's time it\nprint(\"Regular UDTF: Numeric Computation with Small WH and 100x dataset\")\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_udtf.mean_acctbal, compute_mean_stddev_udtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_udtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\n\net=datetime.now()\nprint(f\"Total duration={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cce1aaab-b61d-4bcd-aa10-4f934cf64218",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "This takes around 10-12 seconds to return the results for TPCH_SF100 customer dataset using a Small Warehouse."
  },
  {
   "cell_type": "markdown",
   "id": "85696c9c-bd62-4584-8471-f9632b4fc88d",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "### Numerical Computation using Vectorised UDTF\n\nNext, we will perform the same numerical computation using an end partition method Vectorised UDTF."
  },
  {
   "cell_type": "code",
   "id": "9ae03131-f71d-4972-962c-49fe9d51560d",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_evudtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (mean_acctbal NUMBER(12, 2), stddev_acctbal NUMBER(12, 2))\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy', 'pandas')\nHANDLER='ComputeMeanStddevVUDTF'\nAS $$\nimport numpy as np\nimport pandas as pd\nfrom _snowflake import vectorized\n\nclass ComputeMeanStddevVUDTF:\n    @vectorized(input=pd.DataFrame)\n    def end_partition(self, df):\n        # Extract the c_acctbal column\n        c_acctbal = df['C_ACCTBAL'].values.astype(float)\n        \n        # Handle NaN values by removing them before computation\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            return pd.DataFrame([[np.nan, np.nan]], columns=['mean_acctbal', 'stddev_acctbal'])\n        else:\n            # Compute mean and standard deviation\n            mean_acctbal = np.mean(c_acctbal)\n            stddev_acctbal = np.std(c_acctbal)\n            # Return the results as a DataFrame\n            return pd.DataFrame([[round(mean_acctbal, 2), round(stddev_acctbal, 2)]], columns=['mean_acctbal', 'stddev_acctbal'])\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30f1c4bb-c8f2-4506-9090-339f28741fa3",
   "metadata": {
    "language": "python",
    "name": "cell22",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Now let's time it\nprint(\"End parition method Vectorized UDTF: Numeric Computation with Small WH and 100x dataset\")\nst=datetime.now()\n\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_evudtf.mean_acctbal, compute_mean_stddev_evudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_evudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\n\net=datetime.now()\nprint(f\"Total duration={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6109a127-255a-4340-a931-9d3e330ffe7f",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "- This takes just short of 12 seconds to return the results for TPCH_SF100 customer dataset using a Small Warehouse.\n- So, there is a marginal improvement in performance when using a Vectorised UDTF."
  },
  {
   "cell_type": "markdown",
   "id": "30b9dfd6-5475-4680-b654-1762a5ac3312",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "Next, we will perform the same numerical computation using a process method Vectorised UDTF."
  },
  {
   "cell_type": "code",
   "id": "d3510dab-cfa8-48f0-a0af-01f96959228d",
   "metadata": {
    "language": "python",
    "name": "cell25",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_pvudtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (\n    mean_acctbal NUMBER(12, 2),\n    stddev_acctbal NUMBER(12, 2)\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy', 'pandas')\nHANDLER='ComputeMeanStddevVUDTF'\nAS $$\nimport numpy as np\nimport pandas as pd\nfrom _snowflake import vectorized\n\nclass ComputeMeanStddevVUDTF:\n    @vectorized(input=pd.DataFrame)\n    def process(self, df):\n        # Extract the c_acctbal column\n        c_acctbal = df['C_ACCTBAL'].values.astype(float)\n        \n        # Handle NaN values by removing them before computation\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            return pd.DataFrame([[np.nan, np.nan]], columns=['mean_acctbal', 'stddev_acctbal'])\n        else:\n            # Compute mean and standard deviation\n            mean_acctbal = np.mean(c_acctbal)\n            stddev_acctbal = np.std(c_acctbal)\n            # Return the results as a DataFrame\n            return pd.DataFrame([[round(mean_acctbal, 2), round(stddev_acctbal, 2)]], columns=['mean_acctbal', 'stddev_acctbal'])\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42b3ade8-7b97-4ca4-b43c-99cfd0e4664e",
   "metadata": {
    "language": "python",
    "name": "cell26",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"SELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2abaa3f-4d44-44f8-9430-2dc83e2193fc",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "With the above, you should get the following error:\n- 100357 (P0000): Expected 4096 rows in the output given 4096 rows in the input, but received 1 in function COMPUTE_MEAN_STDDEV_VUDTF with handler ComputeMeanStddevVUDTF\n\n- We need to ensure that the output has the same number of rows as the input, which is different from the batch processing seen in regular UDTFs. We will modify the UDTF to return the computed statistics for each input row, though this will result in redundant values across all rows.\n- This will also severely degrade performance and all in all, it is a bad choice to implement Vectorised UDTFs in this way for this use case."
  },
  {
   "cell_type": "code",
   "id": "738f5f2a-c75e-4e60-afd2-dfef60040766",
   "metadata": {
    "language": "python",
    "name": "cell28",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_pvudtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (\n    mean_acctbal NUMBER(12, 2),\n    stddev_acctbal NUMBER(12, 2)\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy', 'pandas')\nHANDLER='ComputeMeanStddevVUDTF'\nAS $$\nimport numpy as np\nimport pandas as pd\nfrom _snowflake import vectorized\n\nclass ComputeMeanStddevVUDTF:\n    @vectorized(input=pd.DataFrame)\n    def process(self, df):\n        # Extract the c_acctbal column\n        c_acctbal = df['C_ACCTBAL'].values.astype(float)\n        \n        # Handle NaN values by removing them before computation\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            mean_acctbal = np.nan\n            stddev_acctbal = np.nan\n        else:\n            # Compute mean and standard deviation\n            mean_acctbal = np.mean(c_acctbal)\n            stddev_acctbal = np.std(c_acctbal)\n        \n        # Create a DataFrame with the same number of rows as the input\n        result_df = pd.DataFrame({\n            'mean_acctbal': [round(mean_acctbal, 2)] * len(df),\n            'stddev_acctbal': [round(stddev_acctbal, 2)] * len(df)\n        })\n        \n        return result_df\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5e84ade-9bb4-4def-b068-bdcbcfd2f1c7",
   "metadata": {
    "language": "python",
    "name": "cell29",
    "collapsed": false
   },
   "outputs": [],
   "source": "st=datetime.now()\n\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\n\net=datetime.now()\nprint(f\"Total duration={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f9fb3df-ba22-4475-bcd7-17d7536a08a4",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "If you take a look at the [documentation](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-tabular-vectorized#udtfs-with-a-vectorized-process-method), we must remove explicit partitioning for process method vectorisations. This does give a marginal improvement in performance. But overall, this is just bad practice. Head on back to the documentation to understand when it is best to implement a Process Method Vectorised UDTF. Anyway, let's rewrite the call as below:"
  },
  {
   "cell_type": "code",
   "id": "24b764b9-0dd6-481a-95d0-0c2111534bdb",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "st=datetime.now()\n\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal));\n\"\"\").collect()\n\net=datetime.now()\nprint(f\"Total duration={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5db2acf5-399f-4f07-af61-28bb13392cdc",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "### Numerical Computation using variable compute and dataset sizes"
  },
  {
   "cell_type": "code",
   "id": "098ce1b9-de8d-4e5d-97f1-7b5432d42ca9",
   "metadata": {
    "language": "python",
    "name": "cell33",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\n\nprint(\"Using Medium WH and 100x dataset\")\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_udtf.mean_acctbal, compute_mean_stddev_udtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_udtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Regular UDTF={(et-st).total_seconds()}\")\n\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_evudtf.mean_acctbal, compute_mean_stddev_evudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_evudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"End partitioned method vectorized UDTF={(et-st).total_seconds()}\")\n\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Process method vectorized UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0cb905c-afc8-4ed5-b07d-4e653105e2d3",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "print(\"Using Medium WH and 1000x dataset\")\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_udtf.mean_acctbal, compute_mean_stddev_udtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_udtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Regular UDTF={(et-st).total_seconds()}\")\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_evudtf.mean_acctbal, compute_mean_stddev_evudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_evudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"End partitioned method vectorized UDTF={(et-st).total_seconds()}\")\n\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Process method vectorized UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2700de7f-7074-4ca7-875d-276487cbe384",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nprint(\"Using Large WH and 1000x dataset\")\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_udtf.mean_acctbal, compute_mean_stddev_udtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_udtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Regular UDTF={(et-st).total_seconds()}\")\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_evudtf.mean_acctbal, compute_mean_stddev_evudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_evudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"End partitioned method vectorized UDTF={(et-st).total_seconds()}\")\n\n\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_pvudtf.mean_acctbal, compute_mean_stddev_pvudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_pvudtf(c_acctbal));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Process method vectorized UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebb9233e-0aa3-4adb-8175-605c926dcccb",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "### Numerical Computation running concurrent tasks with Worker Processes for Regular UDTF"
  },
  {
   "cell_type": "code",
   "id": "b38a508a-88d6-4393-9afa-6359996c5aed",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "# Let's change the warehouse size as required\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\n\n# Let's check the current size of the warehouse\nsession.sql(\"SHOW WAREHOUSES LIKE 'COMPUTE_WH'\").collect()\nsession.sql('SELECT \"name\", \"size\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))').show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d60d1d0-aa68-4095-be0a-3ffe83d9d6ce",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_cudtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (mean_acctbal NUMBER(12, 2), stddev_acctbal NUMBER(12, 2))\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy', 'joblib')\nHANDLER='ComputeMeanStddevUDTF'\nAS $$\nimport numpy as np\nimport joblib\n\nclass ComputeMeanStddevUDTF:\n    def __init__(self):\n        self.acctbal_values = []\n\n    def process(self, c_acctbal):\n        # Collect each row's c_acctbal value\n        self.acctbal_values.append(float(c_acctbal) if c_acctbal is not None else np.nan)\n\n    def end_partition(self):\n        # Convert to numpy array and handle NaN values\n        c_acctbal = np.array(self.acctbal_values)\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            yield (np.nan, np.nan)\n        else:\n            # Define the function to compute mean and standard deviation\n            def compute_stats(values):\n                mean = np.mean(values)\n                std = np.std(values)\n                return mean, std\n            \n            # Split the data into chunks for parallel processing\n            chunk_size = len(c_acctbal) // joblib.cpu_count()\n            chunks = [c_acctbal[i:i+chunk_size] for i in range(0, len(c_acctbal), chunk_size)]\n            \n            # Run the computation in parallel using joblib\n            results = joblib.Parallel(n_jobs=-1)(joblib.delayed(compute_stats)(chunk) for chunk in chunks)\n            \n            # Combine the results from parallel tasks\n            means, stds = zip(*results)\n            total_mean = np.mean(means)\n            total_std = np.mean(stds)\n            \n            # Yield the final aggregated statistics\n            yield (round(total_mean, 2), round(total_std, 2))\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ceb452dd-f64a-4107-974b-f535001f664b",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "# Now let's time it\nprint(\"Using Small WH and 100x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Medium WH and 100x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Medium WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Large WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4af8c72f-6db5-4754-ba9a-147d70afea49",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "Now let's override the default backend to 'loky'"
  },
  {
   "cell_type": "code",
   "id": "8e7ceccc-f1e0-4623-9bad-7c21cc046b8b",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nCREATE OR REPLACE FUNCTION compute_mean_stddev_cudtf(c_acctbal NUMBER(12, 2))\nRETURNS TABLE (mean_acctbal NUMBER(12, 2), stddev_acctbal NUMBER(12, 2))\nLANGUAGE PYTHON\nRUNTIME_VERSION=3.8\nPACKAGES=('numpy', 'joblib')\nHANDLER='ComputeMeanStddevUDTF'\nAS $$\nimport numpy as np\nimport joblib\njoblib.parallel_backend('loky')\n\nclass ComputeMeanStddevUDTF:\n    def __init__(self):\n        self.acctbal_values = []\n\n    def process(self, c_acctbal):\n        # Collect each row's c_acctbal value\n        self.acctbal_values.append(float(c_acctbal) if c_acctbal is not None else np.nan)\n\n    def end_partition(self):\n        # Convert to numpy array and handle NaN values\n        c_acctbal = np.array(self.acctbal_values)\n        c_acctbal = c_acctbal[~np.isnan(c_acctbal)]\n\n        if len(c_acctbal) == 0:\n            # If there are no valid values left after removing NaNs, return NaNs for all statistics\n            yield (np.nan, np.nan)\n        else:\n            # Define the function to compute mean and standard deviation\n            def compute_stats(values):\n                mean = np.mean(values)\n                std = np.std(values)\n                return mean, std\n            \n            # Split the data into chunks for parallel processing\n            chunk_size = len(c_acctbal) // joblib.cpu_count()\n            chunks = [c_acctbal[i:i+chunk_size] for i in range(0, len(c_acctbal), chunk_size)]\n            \n            # Run the computation in parallel using joblib\n            results = joblib.Parallel(n_jobs=-1)(joblib.delayed(compute_stats)(chunk) for chunk in chunks)\n            \n            # Combine the results from parallel tasks\n            means, stds = zip(*results)\n            total_mean = np.mean(means)\n            total_std = np.mean(stds)\n            \n            # Yield the final aggregated statistics\n            yield (round(total_mean, 2), round(total_std, 2))\n$$\n;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bfec2d9-b03d-472d-843c-9d3074d35e61",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "# Now let's time it\nprint(\"loky backend: Using Small WH and 100x dataset\")\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Medium WH and 100x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF100.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Medium WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Medium'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\nprint(\"Using Large WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Large'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecad21f3-b874-47d0-9b96-c62a18314fd5",
   "metadata": {
    "language": "python",
    "name": "cell46",
    "collapsed": false
   },
   "outputs": [],
   "source": "print(\"Using Snowpark Optimized WH and 1000x dataset\")\nsession.sql(\"CREATE OR REPLACE WAREHOUSE snowpark_opt_wh WITH warehouse_size='MEDIUM' warehouse_type='SNOWPARK-OPTIMIZED';\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8d49057-6216-4f9f-a752-f44cb26b6bd1",
   "metadata": {
    "language": "python",
    "name": "cell43",
    "collapsed": false
   },
   "outputs": [],
   "source": "print(\"Using Snowpark Optimized WH and 1000x dataset\")\nsession.sql(\"CREATE OR REPLACE WAREHOUSE snowpark_opt_wh WITH warehouse_size='MEDIUM' warehouse_type='SNOWPARK-OPTIMIZED';\").collect()\n\nprint(\"Using Snowpark Optimized Medium WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE snowpark_opt_wh SET warehouse_size='Medium'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")\n\n\nprint(\"Using Snowpark Optimized Large WH and 1000x dataset\")\nsession.sql(\"ALTER WAREHOUSE snowpark_opt_wh SET warehouse_size='Large'\").collect()\nst=datetime.now()\nsession.sql(\"\"\"\nSELECT c_nationkey, compute_mean_stddev_cudtf.mean_acctbal, compute_mean_stddev_cudtf.stddev_acctbal\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.customer, TABLE(compute_mean_stddev_cudtf(c_acctbal) OVER (PARTITION BY c_nationkey));\n\"\"\").collect()\net=datetime.now()\nprint(f\"Concurrent tasks with worker process for Regular UDTF={(et-st).total_seconds()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f34e87e-3e0a-4acb-a64d-a3eccaa1ddfa",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": "### Cleanup\n\nThe following code block cleans up what was executed in this lab, by accomplishing the following:\n - Resetting the compute warehouse to SMALL\n - Suspending Snowpark optimzied warehouse"
  },
  {
   "cell_type": "code",
   "id": "54cb03ef-b5de-4f47-a6eb-3938a9b1eb01",
   "metadata": {
    "language": "python",
    "name": "cell45",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE compute_wh SET warehouse_size='Small'\").collect()\nsession.sql(\"ALTER WAREHOUSE snowpark_opt_wh SUSPEND\").collect()",
   "execution_count": null
  }
 ]
}